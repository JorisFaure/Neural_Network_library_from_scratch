import numpy as np
import time
import matplotlib.pyplot as plt

class Neuron:
    def __init__(self, input_size):
        self.weights = np.random.uniform(0.5, 1.0, size=(input_size,))
        self.bias = 0
        self.value = 0
        self.activated_value = 0

    def sigmoid(self):
        return 1 / (1 + np.exp(-self.value))

    def sigmoid_derivative(self):
        return self.activated_value * (1 - self.activated_value)

    def activate(self):
        self.activated_value = self.sigmoid()

    def forward_pass(self, input_vec):
        self.value = np.dot(self.weights, input_vec) + self.bias
        self.activate()

    def cost_derivative_by_layer(self, output_ref) : # (1/2) * 2(y - y_ref)
        return (self.activated_value - output_ref)

class NeuralNetwork:
    def __init__(self, layer_sizes, learning_rate):
        self.total_layers = []
        for i in range(len(layer_sizes) - 1):
            self.total_layers.append([Neuron(layer_sizes[i]) for _ in range(layer_sizes[i + 1])])
        self.learning_rate = learning_rate

    def forward_pass(self, input_vec):
        current_input = input_vec
        for layer in self.total_layers:
            for neuron in layer:
                neuron.forward_pass(current_input)
            current_input = [neuron.activated_value for neuron in layer]
    
    def update_weights_and_biases(self, weight_gradients, bias_gradients):
        i = 0
        for l in range(len(self.total_layers) - 1, -1, -1):
            for neuron in self.total_layers[l]:
                neuron.weights -= self.learning_rate * weight_gradients[i]
                neuron.bias -= self.learning_rate * bias_gradients[i]
                i += 1

    def backpropagation(self, output_vec, input_vec):
        layer_outputs = [input_vec]
        for layer in self.total_layers: # we store all the a(L=n) values
            layer_outputs.append([neuron.activated_value for neuron in layer])

        errors = [] # store the error of each layer
        weight_gradients = [] # store all the weights gradient of each layers (a list of np arrays)
        bias_gradients = [] #store all the bias_gradient of each layers (a list of values)

        
        output_layer = self.total_layers[-1]
        output_errors = [] # store the propagate errors from each output neuron to the previous layer 
        for i, neuron in enumerate(output_layer): # Output backprop
            bias_gradient = neuron.cost_derivative_by_layer(output_vec[i]) * neuron.sigmoid_derivative() # da(L)/dz(L) * dC0/da(L) ***the last term is a(L-1)***
            output_errors.append(bias_gradient) # we use this list for propagating the error
            bias_gradients.append(bias_gradient) # we use this list for the bias update
            weight_gradients.append(np.array(layer_outputs[-2]) * bias_gradient)

        errors.append(output_errors)

        # hidden layers backprop
        for l in range(len(self.total_layers) - 2, -1, -1): #we start from the last hidden layer, to the first hidden layer
            layer = self.total_layers[l]
            next_layer = self.total_layers[l + 1]
            hidden_errors = [] # store the propagate errors from each a(L) neuron to the a(L-1) layer
            for j, neuron in enumerate(layer): # we calculate the propagate sum error from layer L to layer L-1 : dC/dz(L-1) = (Î£(dC0/dz(L)) * w(L)) * (da(L-1)/dz(L-1))
                s = sum(next_layer[k].weights[j] * errors[-1][k] for k in range(len(next_layer))) # error[-1][k] represent the errors of each neuron of the last layer (-1 is the last layer append to the errors list)
                bias_gradient = s * neuron.sigmoid_derivative()
                hidden_errors.append(bias_gradient)
                bias_gradients.append(bias_gradient)
                weight_gradients.append(np.array(layer_outputs[l]) * bias_gradient) # bias_gradient * dC0/da(L-1)
            errors.append(hidden_errors) # we will use the error of the a(l-1) for the next iteration with error[-1]

        return (weight_gradients, bias_gradients)

    def train(self, input_data, output_data, epochs, batch_size):
        errors = []
        for epoch in range(epochs):
            # shuffle the data while keeping pairs of input/output
            permutation = np.random.permutation(len(input_data))
            input_data = input_data[permutation]
            output_data = output_data[permutation]
            
            total_error = 0
            for i in range(0, len(input_data), batch_size):
                batch_input = input_data[i:i+batch_size]
                batch_output = output_data[i:i+batch_size]
                batch_weight_gradients = []
                batch_bias_gradients = []
                for j, sample in enumerate(batch_input):
                    self.forward_pass(sample)
                    gradients = self.backpropagation(batch_output[j], sample)
                    if batch_weight_gradients == []:
                        batch_weight_gradients = gradients[0]
                        batch_bias_gradients = gradients[1]
                    else: # we aggregate all previous gradients with the one of the current sample
                        batch_weight_gradients = [wg + g for wg, g in zip(batch_weight_gradients, gradients[0])]
                        batch_bias_gradients = [bg + g for bg, g in zip(batch_bias_gradients, gradients[1])]
                    # calculate total error
                    total_error += sum((neuron.activated_value - batch_output[j][k])**2 for k, neuron in enumerate(self.total_layers[-1]))
                
                # Average gradients over the batch
                avg_weight_gradients = [wg / batch_size for wg in batch_weight_gradients]
                avg_bias_gradients = [bg / batch_size for bg in batch_bias_gradients]
                self.update_weights_and_biases(avg_weight_gradients, avg_bias_gradients)
            errors.append(total_error / len(input_data))
            if epoch % 1000 == 0:
                # show the MSE every x epoch
                print(f"Epoch {epoch}, MSE: {errors[-1]}")
                
        #create the error plot
        plt.plot(range(epochs), errors, label='Total error')
        plt.xlabel('Epoch')
        plt.ylabel('Mean error')
        plt.title('Error evolution through epoch')
        plt.legend()
        plt.savefig("error_plot.png")
        print("Plot saved as 'error_plot.png'")

    def test(self, input_set, output_set):
        for i, sample in enumerate(input_set):
            self.forward_pass(sample)
            prediction = [round(neuron.activated_value, 2) for neuron in self.total_layers[-1]]
            print(f"Predicted: {prediction} | Reference: {output_set[i]}")



# -----------------------Test of the entire NN-------------------------------
# XOR example
input_set = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
output_set = np.array([[0], [1], [1], [0]])
nn = NeuralNetwork(layer_sizes=[2, 2, 1], learning_rate=0.2)

# Mesure training time
print("Starting training...")
start_time = time.time()
nn.train(input_set, output_set, epochs=15000, batch_size=2)
end_time = time.time()
print("Training over")

training_time = end_time - start_time
print(f"Training time : {training_time:.2f} seconds.")

# Network test
print("\nTest result :")
nn.test(input_set, output_set)